{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import googlemaps\n",
    "import sys\n",
    "import math\n",
    "from math import sin, cos, sqrt, atan2, radians, pi\n",
    "\n",
    "from surprise import KNNWithMeans, SVD, KNNBasic\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import geopy.distance\n",
    "from geopy.distance import geodesic\n",
    "from geopy import distance\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_PATH = './dataset_test/rating_avaliacao.csv'\n",
    "#POIS_PATH = './dataset/pois.csv'\n",
    "POIS_PATH = './dataset_test/all_pois.csv'\n",
    "USER_PATH = './dataset_test/user_avaliacao.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommendations:\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    def __init__(self, user_data='', rating_data='', data_frame='', poi_data=''):\n",
    "        \n",
    "        if rating_data:\n",
    "            ''' userId,poiId,rating '''\n",
    "            reader = Reader(sep=',')\n",
    "            self.ratings = Dataset.load_from_file(rating_data, reader)\n",
    "            self.trainset = self.ratings.build_full_trainset()\n",
    "            self.sim_options = {'name': 'cosine','user_based': False}\n",
    "            #self.df_ratings = pd.read_csv(rating_data, low_memory=False, names=['userId','latitude','longitude','poiId','rating'])\n",
    "            self.df_ratings = pd.read_csv(rating_data, low_memory=False, names=['userId','poiId','rating'])\n",
    "        elif not data_frame.empty:\n",
    "            reader = Reader(rating_scale=(0, 5))\n",
    "            self.ratings = Dataset.load_from_df(data_frame[['userId', 'poiId', 'rating']], reader)\n",
    "            self.trainset = self.ratings.build_full_trainset()\n",
    "            self.sim_options = {'name': 'cosine','user_based': False}\n",
    "        if poi_data:\n",
    "            ''' poiId,latitude,longitude,name,preferenceid,preference '''\n",
    "            self.pois = pd.read_csv(poi_data, low_memory=False)\n",
    "            self.pois = pd.DataFrame(self.pois, columns=['poiId','latitude','longitude','name','preference'])\n",
    "            #self.csv_reader = csv.reader(self.pois, delimiter=',') \n",
    "        if user_data:\n",
    "            ''' userId,name,latitude,longitude,id_preferencia,preference '''\n",
    "            self.users = pd.read_csv(user_data, low_memory=False)\n",
    "            #self.csv_reader = csv.reader(pois, delimiter=',') \n",
    "            \n",
    " \n",
    "    def random_group(self, n):\n",
    "        ''' Gera um grupo randomico de tamanho n\n",
    "            Retorna o grupo\n",
    "        '''\n",
    "        self.users_list = list(self.users['userId'])  \n",
    "        random_group = random.sample(self.users_list,n)\n",
    "       \n",
    "        '''verifica se possui algum user repetido no grupo\n",
    "        se houver randomiza novamente... A função len e set verificam\n",
    "        se os tamanhos da lista e conjunto são iguais'''\n",
    "        if len(random_group) != len(set(random_group)):\n",
    "            while len(random_group) != len(set(random_group)):    \n",
    "                random_group = random.sample(self.users_list,n)\n",
    "        \n",
    "        random_group = [81, 151, 91]\n",
    "        #[81, 151, 91]\n",
    "        #[131, 231, 211, 171, 121]\n",
    "        #[51, 161, 141, 101, 61, 71, 181, 191, 111, 201]\n",
    "       \n",
    "\n",
    "        return random_group\n",
    "               \n",
    "            \n",
    "    def set_k(self, k_value=5):\n",
    "        ''' Sets the prediction algorithm used. The default is SVD.\n",
    "        '''  \n",
    "        if k_value:\n",
    "            algo = KNNWithMeans(k=k_value, sim_options=self.sim_options)\n",
    "            self.algo = algo\n",
    "            self.algo.fit(self.trainset)\n",
    "        else:\n",
    "            #algo = SVD(random_state=33)\n",
    "            algo = SVD()\n",
    "            self.algo = algo\n",
    "            self.algo.fit(self.trainset)\n",
    "    \n",
    "    def set_testset(self, group):\n",
    "        ''' Define quais itens são considerados itens candidatos para um grupo, se os membros forem fornecidos.\n",
    "            Atualiza testset.\n",
    "            Retorna o conjunto de testes atualizado.\n",
    "        '''\n",
    "        if group:\n",
    "            ''' trainset.ur é uma tuple de (item_inner_id, rating)\n",
    "                As avaliações dos usuários. Este é um dicionário contendo listas de tuplas da forma \n",
    "                (item_inner_id, rating). \n",
    "                As chaves são ids internos do usuário.\n",
    "            ''' \n",
    "            user_ratings = self.trainset.ur\n",
    "            pois_ids = list(self.pois['poiId'])\n",
    "\n",
    "            # A média de todas as classificações μ.\n",
    "            global_mean=self.trainset.global_mean\n",
    "            print(\"global_mean: {}\".format(global_mean))\n",
    "            my_testset = []\n",
    "            \n",
    "            for user in group:\n",
    "                iuid = self.trainset.to_inner_uid(str(user))\n",
    "                for item in pois_ids:\n",
    "                    is_in = False\n",
    "                    for rating in user_ratings[iuid]:\n",
    "                        if int(item) == int(self.trainset.to_raw_iid(int(rating[0]))):\n",
    "                            is_in = True\n",
    "                            break\n",
    "                    if not is_in:\n",
    "                        my_tuple = (str(user),str(item),global_mean)\n",
    "                        my_testset.append(my_tuple)\n",
    "                        \n",
    "            self.testset = my_testset\n",
    "        else:\n",
    "            testset = self.trainset.build_anti_testset()\n",
    "            self.testset = testset\n",
    "            \n",
    "        '''for x in self.testset:\n",
    "            print(\"testset: {}\".format(x))'''\n",
    "        \n",
    "        #print(\"acurracy:{}\".format(accuracy.rmse(self.testset)))\n",
    "\n",
    "        return self.testset\n",
    "\n",
    "\n",
    "    def predict_ratings(self,group=''):\n",
    "        ''' Predicts ratings for all pairs (u, i) that are NOT in the training set. In other words, \n",
    "            predicts ratings from candidate items.\n",
    "            Sets predictions\n",
    "        '''\n",
    "        testset = self.set_testset(group)\n",
    "        predictions = self.algo.test(testset)\n",
    "        self.predictions = predictions\n",
    "        \n",
    "        '''for x in self.predictions:\n",
    "            print(\"predictions:{}\".format(x))'''\n",
    "        \n",
    "    \n",
    "    def set_profile_pois(self, group):\n",
    "        ''' Items that were rated for AT LEAST ONE group member will compound the group profile.\n",
    "            Sets group_sparse_mtx, profile_pois\n",
    "            Os itens classificados para PELO MENOS UM membro do grupo irão compor o perfil do grupo.\n",
    "            Define group_sparse_mtx, profile_pois\n",
    "        '''\n",
    "        metadata = pd.read_csv(RATINGS_PATH, low_memory=False, names=['userId', 'poiId', 'rating'])\n",
    "        \n",
    "        metadata_filtered = metadata[metadata.userId.isin(group)]\n",
    "\n",
    "        self.group_sparse_mtx = pd.pivot_table(metadata_filtered, values='rating', index=['userId'], columns=['poiId'], fill_value=0)\n",
    "        self.profile_pois = list(self.group_sparse_mtx)   \n",
    "        \n",
    "        \n",
    "    def set_candidate_pois(self):\n",
    "        ''' Items that were NOT rated by any group member will be candidates for recommendation.\n",
    "            Sets group_sparse_mtx, profile_pois\n",
    "            Os itens que NÃO foram avaliados por ninguém do grupo serão candidatos para recomendação.\n",
    "            Define group_sparse_mtx, profile_pois\n",
    "        '''\n",
    "        candidate_pois = []\n",
    "        for poi in self.pois.iterrows():\n",
    "        #     get the Id of each item in items dataframe\n",
    "            if poi[1].values[0] not in self.profile_pois:\n",
    "                candidate_pois.append(poi[1].values[0])\n",
    "        self.candidate_pois = candidate_pois\n",
    "        #print(\"candidate_pois: {}\".format(self.candidate_pois))\n",
    "        \n",
    "    def calc_similarity_matrix(self):\n",
    "        ''' Calculates the items similarity matrix using cosine similarity. This function was developed based on MovieLens dataset, using titles and genres.\n",
    "            Sets cosine_sim_pois_name, cosine_sim_pois_preference\n",
    "        '''\n",
    "        #print(self.pois.columns.tolist())\n",
    "\n",
    "        #Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        \n",
    "        #Replace NaN with an empty string\n",
    "        self.pois['name'] = self.pois['name'].fillna('')\n",
    "        self.pois['preference'] = self.pois['preference'].fillna('')\n",
    "        \n",
    "        #Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "        tfidf_matrix_name = tfidf.fit_transform(self.pois['name'])\n",
    "        tfidf_matrix_preference = tfidf.fit_transform(self.pois['preference'])\n",
    "        \n",
    "        #print(\"tfidf_matrix_name:\\n {}\".format(tfidf_matrix_name))\n",
    "        \n",
    "        #Compute the cosine similarity matrix\n",
    "        self.cosine_sim_pois_name = cosine_similarity(tfidf_matrix_name, tfidf_matrix_name)\n",
    "        self.cosine_sim_pois_preference = cosine_similarity(tfidf_matrix_preference, tfidf_matrix_preference)\n",
    "        \n",
    "        \n",
    "    def distance_matrix(self, group):\n",
    "        ''' Função que calcula a distancia dos usuarios do grupo ao pontos de interesse\n",
    "        '''\n",
    "        \n",
    "        ''' Pega a latitude e longitude dos usuarios do grupo\n",
    "            Depois retira do dataframe as duplicatas e pegando apenas as colunas necessárias\n",
    "            Takes the latitude and longitude of users in the group\n",
    "            Then remove the duplicates from the dataframe and take only the necessary columns'''\n",
    "       \n",
    "        self.df_group = self.users[self.users['userId'].isin(group)]\n",
    "        self.df_group = self.df_group.drop_duplicates(subset = [\"userId\"]) \n",
    "        self.df_group = self.df_group[['userId','latitude','longitude']] \n",
    "        self.df_group['userId'] = self.df_group['userId'].astype(int)\n",
    "         \n",
    "        ''' somente os pois que foram avaliados por PELO MENOS UM membro do grupo  '''\n",
    "        pois_filter = self.pois[self.pois['poiId'].isin(self.profile_pois)]\n",
    "        \n",
    "        \n",
    "        ''' #My API key\n",
    "        ''' \n",
    "        gmaps = googlemaps.Client(key='AIzaSyC97oj_73Oab6zrUkHfWH-gq7zF2omHkOo')\n",
    "\n",
    "        '''  Create Dataframe de retorno \n",
    "        '''\n",
    "        cols=['userId', 'poiId', 'distance']\n",
    "        m_distance = pd.DataFrame(columns=cols)\n",
    "        \n",
    "        for index, group_row in self.df_group.iterrows():\n",
    "            for index, poi_row in pois_filter.iterrows():\n",
    "            #for index, poi_row in self.pois.iterrows():\n",
    "                \n",
    "                '''print('Usuario: {}, {}, {}'.format(group_row['userId'],group_row['latitude'],group_row['longitude']))\n",
    "                print(\"POI: {} {}, {}, {}\". format(poi_row['poiId'], poi_row['name'], poi_row['latitude'], poi_row['longitude']))'''\n",
    "                                \n",
    "                 #COLOCA AS LOCALIZAÇÕES NAS VARIAVEIS\n",
    "                \n",
    "                origin = group_row['latitude'], group_row['longitude']\n",
    "                destination = poi_row['latitude'], poi_row['longitude']\n",
    "                \n",
    "                    \n",
    "                try:\n",
    "                     #DISTANCE CALCULATION API -> Params: origem, destino, mode, language                   \n",
    "                    query_distance = gmaps.distance_matrix(origin, destination)\n",
    "                    #query_distance = ''\n",
    "                    \n",
    "                     #Get A DISTANCIA EM METROS DO JSON DE RETORNO DA CONSULTA\n",
    "                    distance = query_distance['rows'][0]['elements'][0]['distance']['value']\n",
    "                    '''print('distance: {}'.format(distance))\n",
    "                    print(\"\\n\")'''\n",
    "                    \n",
    "               \n",
    "                except:\n",
    "                    distance = ''\n",
    "                    print(\"Não foi possivel calcular a distancia\")\n",
    "                   \n",
    "                ''' #CRIA UMA MATRIZ TEMPORARIA PARA DEPOIS PASSAR PARA A MINHA MATRIZ\n",
    "                '''  \n",
    "                info_temp = [group_row['userId'].astype(int), poi_row['poiId'], distance]\n",
    "                temp = pd.DataFrame([info_temp], columns=cols)\n",
    "                \n",
    "                ''' #PASSA AS INFORMAÇÕES PARA MATRIZ DE RETORNO INGNORANDO O INDEX DA MATRIZ TEMPORARIA\n",
    "                ''' \n",
    "                m_distance = m_distance.append(temp, ignore_index=True)\n",
    "                \n",
    "        \n",
    "        ''' #EXPORTA A MATRIZ DAS DISTANCIAS PARA UM ARQUIVO CSV\n",
    "        ''' \n",
    "        export = m_distance.to_csv(r'./dataset/matrix_distance.csv',index=False)\n",
    "        \n",
    "        return m_distance\n",
    "\n",
    "    \n",
    "    def calculate_matrix_mpd(self, group_filled_mtx, distance_mtx):\n",
    "        \n",
    "        ''' Change type'''\n",
    "        distance_mtx['userId'] = distance_mtx['userId'].astype(int)\n",
    "        distance_mtx['poiId'] = distance_mtx['poiId'].astype(int)\n",
    "        distance_mtx['distance'] = pd.to_numeric(distance_mtx['distance'])\n",
    "        #group_distance_mtx['distance'] = group_distance_mtx['distance'].astype(float)\n",
    "        \n",
    "        ''' Pivota a matrix de distancia'''\n",
    "        distance_pivot_mtx = pd.pivot_table(distance_mtx, values='distance', index=['userId'], columns=['poiId'], fill_value=0)\n",
    "\n",
    "        ''' Multiplica a matrix distancia pela matrix preferencia'''\n",
    "        group_mpd = []\n",
    "        group_mpd = group_filled_mtx*distance_pivot_mtx\n",
    "        \n",
    "        return group_mpd\n",
    "        \n",
    "    \n",
    "    def apply_aggregation_strategy(self, group_mpd, technique = 'AWM'):\n",
    "        ''' Sets the aggregation technique applied.\n",
    "            Returns the group profile aggregated.\n",
    "        '''        \n",
    "        values = []\n",
    "        labels = []\n",
    "        for i in range(0,len(list(group_mpd))):\n",
    "            my_col = group_mpd.iloc[ : ,i]\n",
    "            label = my_col.name\n",
    "            my_col = list(my_col)\n",
    "\n",
    "            labels.append(label)\n",
    "            values.append(0.0)\n",
    "\n",
    "            if technique is 'LM':\n",
    "                values.append( float(min(my_col)) )\n",
    "            elif technique is 'MP':\n",
    "                values.append( float(max(my_col)) )\n",
    "            elif technique is 'AV':\n",
    "                values.append( float( sum(my_col) / len(my_col) ) )\n",
    "            else:\n",
    "                if float(min(my_col)) <= 2 :\n",
    "                    values.append( float(min(my_col)) )\n",
    "                else:\n",
    "                    values.append( float( sum(my_col) / len(my_col) ) )\n",
    "\n",
    "        print('\\n-- -- --  -- > Aggregation Technique chosen: {}\\n'.format(technique))\n",
    "        \n",
    "        agg_group_profile = pd.DataFrame(index=[900], columns=labels)\n",
    "\n",
    "        for i in range(0,len(list(agg_group_profile))):\n",
    "            agg_group_profile.iloc[0, i] = values[i]\n",
    "\n",
    "        agg_group_profile = agg_group_profile.round(decimals=3)\n",
    "        #print(\"agg_group_profile\\n {}\".format(agg_group_profile))\n",
    "\n",
    "        return agg_group_profile\n",
    "    \n",
    "    def get_similar_items(self, references, name_weight=0.8, k=10):\n",
    "        ''' Searches for the top-k most similar pois in candidate pois to a given reference list. \n",
    "            This function is based on MovieLens dataset.\n",
    "            Returns a list of pois.\n",
    "        '''\n",
    "        recs = []\n",
    "        for poi in references:\n",
    "            # Get the pairwsie similarity scores of all pois with that poi\n",
    "            poi_idx = int(self.pois[self.pois['poiId']==poi['poiId']].index[0])\n",
    "            sim_scores_name = list(enumerate(self.cosine_sim_pois_name[poi_idx]))\n",
    "            sim_scores_preferences = list(enumerate(self.cosine_sim_pois_preference[poi_idx]))\n",
    "            \n",
    "            # Calculate total similarity based on title and genres\n",
    "            total_sim_score = []\n",
    "            for i in range(len(sim_scores_name)):\n",
    "                aux = (sim_scores_name[i][1]*name_weight) + (sim_scores_preferences[i][1]*(1-name_weight))\n",
    "                total_sim_score.append((i, aux))\n",
    "                \n",
    "            # Sort the pois based on the similarity scores\n",
    "            total_sim_score = sorted(total_sim_score, key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            candidates_sim_score = []\n",
    "            for sim_poi in total_sim_score:\n",
    "                if self.pois.loc[sim_poi[0]].values[0] not in self.profile_pois:\n",
    "                    candidates_sim_score.append(sim_poi)\n",
    "            \n",
    "            # Get the scores of the top-k most similar pois\n",
    "            k = k + 1\n",
    "            candidates_sim_score = candidates_sim_score[1:k]\n",
    "            recs.append(candidates_sim_score)\n",
    "            \n",
    "        return recs\n",
    "    \n",
    "    def get_relevance_score(self, recs, references):\n",
    "        ''' Calculates the relevance of recommendations.\n",
    "            Creates a dictionary for better manipulation of data, containing: \n",
    "                poi_id, poi_name, poi_similarity and poi_relevance. \n",
    "                This function is based on MovieLens dataset.\n",
    "            Returns a dict sorted by movie_relevance.\n",
    "        '''\n",
    "        count = 0\n",
    "        recs_dict = []\n",
    "        for reference in references:\n",
    "            for poi in recs[count]:\n",
    "                aux = {}\n",
    "                poi_id = self.pois.loc[poi[0]].values[0]\n",
    "                poi_latitude = self.pois.loc[poi[0]].values[1]\n",
    "                poi_longitude = self.pois.loc[poi[0]].values[2]\n",
    "                poi_name = self.pois.loc[poi[0]].values[3]\n",
    "                poi_preferences = self.pois.loc[poi[0]].values[4]\n",
    "                poi_similarity = poi[1]\n",
    "                poi_relevance = round(((reference['rating']/5.0)+poi_similarity)/2, 3)\n",
    "\n",
    "                aux['poi_id'] = poi_id\n",
    "                aux['poi_name'] = poi_name\n",
    "                aux['poi_preferences'] = poi_preferences\n",
    "                aux['poi_similarity'] = poi_similarity\n",
    "                aux['poi_relevance'] = poi_relevance\n",
    "                aux['poi_latitude'] = poi_latitude\n",
    "                aux['poi_longitude'] = poi_longitude\n",
    "\n",
    "                recs_dict.append(aux)\n",
    "\n",
    "                #print('\\tSim: {},\\trelevance: {},\\tpoiId: {},\\tname: {}'.format(aux['poi_similarity'], aux['poi_relevance'], aux['poi_id'], aux['poi_name']))\n",
    "\n",
    "            count=count+1\n",
    "\n",
    "        recs_dict = sorted(recs_dict, key = lambda i: i['poi_relevance'],reverse=True)\n",
    "\n",
    "        return recs_dict\n",
    "    \n",
    "    def calc_distance_item_in_list(self, item, this_list, title_weight=0.8):\n",
    "        ''' Calculates the total distance of an item in relation to a given list.\n",
    "            Returns the total distance.\n",
    "        '''\n",
    "        idx_i = int(self.pois[self.pois['poiId']==int(item['poi_id'])].index[0])\n",
    "\n",
    "        total_dist = 0\n",
    "        for movie in this_list:\n",
    "            \n",
    "            idx_j = int(self.pois[self.pois['poiId']==int(movie['poi_id'])].index[0])\n",
    "\n",
    "            sim_i_j = (self.cosine_sim_pois_name[idx_i][idx_j]*title_weight) + (self.cosine_sim_pois_preference[idx_i][idx_j]*(1-title_weight))\n",
    "            dist_i_j = 1 - sim_i_j\n",
    "            total_dist = total_dist + dist_i_j\n",
    "\n",
    "        result = total_dist/len(this_list)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def calc_diversity_score(self, actual_list, candidates_list, alfa=0.5):\n",
    "        '''\n",
    "            This function implemented here was based on MARIUS KAMINSKAS and DEREK BRIDGE paper: Diversity, Serendipity, Novelty, and Coverage: A Survey and Empirical Analysis of Beyond-Accuracy Objectives in Recommender Systems\n",
    "                \n",
    "                func(i,R) = (relevance[i]*alfa) + (dist_i_R(i,R)*(1-alfa))\n",
    "\n",
    "            Calculates the diversity score that an item represents to a given list.\n",
    "            Returns a dict with calculated values.\n",
    "        '''\n",
    "        diversity_score = []\n",
    "        count = 0\n",
    "\n",
    "        for item in candidates_list:\n",
    "\n",
    "            aux = {}\n",
    "            dist_item_R = self.calc_distance_item_in_list(item=item, this_list=actual_list)\n",
    "            aux['div_score'] = (item['poi_relevance']*alfa) + (dist_item_R*(1-alfa))\n",
    "            aux['idx'] = count\n",
    "            diversity_score.append(aux)\n",
    "            count = count + 1\n",
    "\n",
    "        return diversity_score\n",
    "\n",
    "\n",
    "    def diversify_recs_list(self, recs, k=10):\n",
    "        '''\n",
    "            This function implemented here was based on MARIUS KAMINSKAS and DEREK BRIDGE paper: Diversity, Serendipity, Novelty, and Coverage: A Survey and Empirical Analysis of Beyond-Accuracy Objectives in Recommender Systems\n",
    "        \n",
    "                The Greedy Reranking Algorithm.\n",
    "\n",
    "            Given a list, returns another list with top-k items diversified based on the Greedy algorithm.\n",
    "        '''\n",
    "        diversified_list = []\n",
    "        \n",
    "        while len(diversified_list) < k:\n",
    "            if len(diversified_list) == 0:\n",
    "                diversified_list.append(recs[0])\n",
    "                recs.pop(0)\n",
    "            else:\n",
    "                diversity_score = self.calc_diversity_score(actual_list=diversified_list, candidates_list=recs)\n",
    "                diversity_score = sorted(diversity_score, key = lambda i: i['div_score'],reverse=True)\n",
    "                #  Add the item that maximize diversity in the list \n",
    "                item = diversity_score[0]\n",
    "                diversified_list.append(recs[item['idx']])\n",
    "                #  Remove this item from the candidates list\n",
    "                recs.pop(item['idx'])\n",
    "    \n",
    "        return diversified_list\n",
    "\n",
    "\n",
    "    def diversify_recs_list_bounded_random(self, recs, k=10):\n",
    "        '''\n",
    "            This function implemented here was based on KEITH BRADLEY and BARRY SMYTH paper: Improving Recommendation Diversity\n",
    "                \n",
    "                The Bounded Random Selection Algorithm.\n",
    "\n",
    "            Returns a list with top-k items diversified based on the Bounded Random algorithm.\n",
    "        '''\n",
    "        diversified_list = random.sample(recs,k)\n",
    "\n",
    "        return diversified_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-->  Initializing...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "-->  Group members: [81, 151, 91]\n",
      "global_mean: 3.652173913043478\n",
      "\n",
      "\n",
      "-->  Calculating distance matrix...\n",
      "\n",
      "\n",
      "-->  Calculating items similarity matrix...\n",
      "\n",
      "\n",
      "-->  Calculating group matrix FILLED...\n",
      "\n",
      "\n",
      "-->  Calculating distance vs. matrix filled (MPD)...\n",
      "\n",
      "\n",
      "-->  Applying aggregation technique...\n",
      "\n",
      "-- -- --  -- > Aggregation Technique chosen: LM\n",
      "\n",
      "\n",
      "\n",
      "-->  Creating group preferences dict...\n",
      "\n",
      "\n",
      "-->  Calculating recommendations...\n",
      "\n",
      "\n",
      "-->  The top-10 STANDARD recs are:\n",
      "\n",
      "poiId: 609, relevance: 4534.6, name:The Twist, description:nightclub\n",
      "poiId: 610, relevance: 4534.6, name:30 Segundos, description:nightclub\n",
      "poiId: 612, relevance: 4534.6, name:Blind Experience Club, description:nightclub\n",
      "poiId: 1, relevance: 4534.5, name:Oxum Casa de Arte, description:arts_centre\n",
      "poiId: 2, relevance: 4534.5, name:Galeria de Arte Paulo Darze, description:arts_centre\n",
      "poiId: 3, relevance: 4534.5, name:Alianca francesa, description:arts_centre\n",
      "poiId: 4, relevance: 4534.5, name:Instituto Goethe, description:arts_centre\n",
      "poiId: 5, relevance: 4534.5, name:Caixa Cultural, description:arts_centre\n",
      "poiId: 6, relevance: 4534.5, name:Galeria Pierre Verger, description:arts_centre\n",
      "poiId: 129, relevance: 4534.5, name:Barraca Bela Vista, description:bar\n",
      "poiId: 462, relevance: 3559.7, name:Bobs, description:fast_food\n",
      "poiId: 398, relevance: 3559.3, name:Jodies Pizza, description:fast_food\n",
      "poiId: 400, relevance: 3559.3, name:Gatinho Lanchonette, description:fast_food\n",
      "poiId: 401, relevance: 3559.3, name:Lanchonete Estrelao, description:fast_food\n",
      "poiId: 402, relevance: 3559.3, name:McDonalds, description:fast_food\n",
      "poiId: 403, relevance: 3559.3, name:Lanchonete Tudo de Bom, description:fast_food\n",
      "poiId: 404, relevance: 3559.3, name:CSP Lanches, description:fast_food\n",
      "poiId: 406, relevance: 3559.3, name:McDonalds, description:fast_food\n",
      "poiId: 407, relevance: 3559.3, name:Buonna Pizza, description:fast_food\n",
      "poiId: 408, relevance: 3559.3, name:Sandwich Hall, description:fast_food\n",
      "\n",
      "\n",
      "-->  The top-10 GREEDY DIVERSIFIED recs are:\n",
      "\n",
      "poiId: 609, relevance: 4534.6, name:The Twist, description:nightclub\n",
      "poiId: 1, relevance: 4534.5, name:Oxum Casa de Arte, description:arts_centre\n",
      "poiId: 610, relevance: 4534.6, name:30 Segundos, description:nightclub\n",
      "poiId: 129, relevance: 4534.5, name:Barraca Bela Vista, description:bar\n",
      "poiId: 612, relevance: 4534.6, name:Blind Experience Club, description:nightclub\n",
      "poiId: 3, relevance: 4534.5, name:Alianca francesa, description:arts_centre\n",
      "poiId: 4, relevance: 4534.5, name:Instituto Goethe, description:arts_centre\n",
      "poiId: 5, relevance: 4534.5, name:Caixa Cultural, description:arts_centre\n",
      "poiId: 6, relevance: 4534.5, name:Galeria Pierre Verger, description:arts_centre\n",
      "poiId: 2, relevance: 4534.5, name:Galeria de Arte Paulo Darze, description:arts_centre\n",
      "\n",
      "\n",
      "-->  The top-10 RANDOM DIVERSIFIED recs are:\n",
      "\n",
      "poiId: 144, relevance: 1659.2, name:Bar abafadinho do Nal, description:bar\n",
      "poiId: 403, relevance: 3559.3, name:Lanchonete Tudo de Bom, description:fast_food\n",
      "poiId: 134, relevance: 3109.0, name:Mua, description:bar\n",
      "poiId: 704, relevance: 1760.436, name:Igreja da Ordem Terceira do Carmo, description:place_of_worship\n",
      "poiId: 827, relevance: 3021.5, name:Restaurante O Rei da Carne do Sol, description:restaurant\n",
      "poiId: 578, relevance: 2655.3, name:Ondina Acai, description:ice_cream\n",
      "poiId: 140, relevance: 1659.2, name:O Lider, description:bar\n",
      "poiId: 129, relevance: 3109.0, name:Barraca Bela Vista, description:bar\n",
      "poiId: 739, relevance: 2741.259, name:Igreja Batista Lirio dos Vales, description:place_of_worship\n",
      "poiId: 828, relevance: 1573.747, name:Multiplus, description:restaurant\n"
     ]
    }
   ],
   "source": [
    "recommendations = Recommendations(user_data=USER_PATH, rating_data=RATINGS_PATH, poi_data=POIS_PATH)\n",
    "\n",
    "print(\"\\n-->  Initializing...\")\n",
    "recommendations.set_k()\n",
    "\n",
    "my_group = recommendations.random_group(10)\n",
    "print('\\n-->  Group members: {}'.format(my_group))\n",
    "\n",
    "recommendations.predict_ratings(group=my_group)\n",
    "\n",
    "recommendations.set_profile_pois(group=my_group)\n",
    "recommendations.set_candidate_pois()\n",
    "\n",
    "print(\"\\n\\n-->  Calculating distance matrix...\")\n",
    "# Calculo com API da Google\n",
    "distance_mtx = recommendations.distance_matrix(group=my_group)\n",
    "\n",
    "\n",
    "print(\"\\n\\n-->  Calculating items similarity matrix...\")\n",
    "recommendations.calc_similarity_matrix()\n",
    "\n",
    "\n",
    "print(\"\\n\\n-->  Calculating group matrix FILLED...\")\n",
    "group_filled_mtx = recommendations.group_sparse_mtx.copy()\n",
    "\n",
    "\n",
    "for index, row in group_filled_mtx.iterrows():\n",
    "    for col in list(group_filled_mtx):\n",
    "        if(group_filled_mtx.loc[index,col] == 0):\n",
    "            aux = list(filter(lambda x: x.uid==str(index) and x.iid==str(col), recommendations.predictions))\n",
    "            #group_filled_mtx.loc[index,col] = aux[0]\n",
    "            group_filled_mtx.loc[index,col] = aux[0].r_ui \n",
    "\n",
    "# A matrix densa com os valores das predições\n",
    "group_filled_mtx = group_filled_mtx.round(decimals=3)\n",
    "\n",
    "\n",
    "print(\"\\n\\n-->  Calculating distance vs. matrix filled (MPD)...\")\n",
    "group_mpd = recommendations.calculate_matrix_mpd(group_filled_mtx, distance_mtx)\n",
    "\n",
    "\n",
    "print(\"\\n\\n-->  Applying aggregation technique...\")\n",
    "#MP, LM, AV, AWM\n",
    "agg_group_profile = recommendations.apply_aggregation_strategy(group_mpd, 'LM')\n",
    "\n",
    "\n",
    "print(\"\\n\\n-->  Creating group preferences dict...\")\n",
    "group_pref_dict = []\n",
    "for col in list(agg_group_profile):\n",
    "    my_dict = {}\n",
    "    my_dict['rating'] = agg_group_profile.loc[900,col]\n",
    "    my_dict['poiId'] = col\n",
    "    group_pref_dict.append(my_dict)\n",
    "\n",
    "group_pref_dict = sorted(group_pref_dict, key = lambda i: i['rating'],reverse=True)\n",
    "\n",
    "\n",
    "references = group_pref_dict[0:20]\n",
    "\n",
    "print(\"\\n\\n-->  Calculating recommendations...\")\n",
    "recs = recommendations.get_similar_items(references)\n",
    "candidates_list = recommendations.get_relevance_score(recs=recs, references=references)\n",
    "\n",
    "\n",
    "print(\"\\n\\n-->  The top-10 STANDARD recs are:\\n\")\n",
    "for poi in candidates_list[0:20]:\n",
    "    print('poiId: {}, relevance: {}, name:{}, description:{}'.format(poi['poi_id'], poi['poi_relevance'], poi['poi_name'], poi['poi_preferences']))\n",
    "\n",
    "    \n",
    "\n",
    "my_candidates = candidates_list.copy()\n",
    "final_recs_greedy = recommendations.diversify_recs_list(recs=my_candidates)\n",
    "print(\"\\n\\n-->  The top-10 GREEDY DIVERSIFIED recs are:\\n\")\n",
    "for item in final_recs_greedy:\n",
    "    print('poiId: {}, relevance: {}, name:{}, description:{}'.format(item['poi_id'], item['poi_relevance'], item['poi_name'], item['poi_preferences']))\n",
    "\n",
    "\n",
    "my_candidates = candidates_list.copy()\n",
    "final_recs_random = recommendations.diversify_recs_list_bounded_random(recs=my_candidates)\n",
    "print(\"\\n\\n-->  The top-10 RANDOM DIVERSIFIED recs are:\\n\")\n",
    "for item in final_recs_random:\n",
    "    print('poiId: {}, relevance: {}, name:{}, description:{}'.format(item['poi_id'], item['poi_relevance'], item['poi_name'], item['poi_preferences']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('ProgramData': virtualenv)",
   "language": "python",
   "name": "python37664bitprogramdatavirtualenvf530f6b9bb5d4f2cad2c5dc94ccc90d8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
